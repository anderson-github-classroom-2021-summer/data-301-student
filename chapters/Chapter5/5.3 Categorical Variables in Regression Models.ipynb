{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "5.3 Categorical Variables in Regression Models.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/pods/blob/master/05-Regression-Models/5.3%20Categorical%20Variables%20in%20Regression%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHPKDZB9QgaM",
        "colab_type": "text"
      },
      "source": [
        "# 5.3 Categorical Features in Regression Models\n",
        "\n",
        "So far, we have fit linear and $k$-nearest neighbors regression models to data where all of the features are quantitative. But what if all or some of the features are categorical? In theory, the solution is simple: we simply transform the categorical variables into quantitative variables using dummy (i.e., one-hot) encoding, following the process in Chapter 3. However, in practice, some care is needed to ensure that the categorical variables are transformed in a consistent way between the training and the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL4uMwkXQgaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_housing = pd.read_csv(\"http://dlsun.github.io/pods/data/AmesHousing.txt\", sep=\"\\t\")\n",
        "df_housing.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgT-LLRPQgaU",
        "colab_type": "text"
      },
      "source": [
        "## One Categorical Feature\n",
        "\n",
        "Let's develop some intuition about the predictions that a regression model will make when there is a single categorical feature. First, suppose we train a linear regression model to predict house price from the neighborhood the house is in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTKRGhAjQgaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = df_housing[[\"Neighborhood\"]] # need 2D array for sklearn\n",
        "y = df_housing[\"SalePrice\"]\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "X_dummies = enc.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_dummies, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei7SJrH3Qgae",
        "colab_type": "text"
      },
      "source": [
        "A regression model with just a single feature, **Neighborhood**, will predict the same price for all houses in the same neighborhood. What is that predicted value? We can obtain it by applying the `OneHotEncoder` to a list of the unique neighborhoods in the data set and passing this to `model.predict()`.\n",
        "\n",
        "One way to obtain a list of the unique neighborhoods is inside the encoder itself, under the attribute `.categories_`. We convert this to a 2D-array to be compatible with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxIWrGDsQgag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = pd.Series(enc.categories_[0], name=\"Neighborhood\").to_frame()\n",
        "X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuFNvJixQgak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(enc.transform(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7TJj5ffQgas",
        "colab_type": "text"
      },
      "source": [
        "It is a bit hard to tell which prediction corresponds to which neighborhood. Let's put these numbers into a `Series`, indexed by the neighborhood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65micH-yQgat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series(\n",
        "    model.predict(enc.transform(X_test)),\n",
        "    index=X_test[\"Neighborhood\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdybOz4IQgay",
        "colab_type": "text"
      },
      "source": [
        "Could we have obtained these predictions some other way, without going through the trouble of fitting a linear regression model? Intuitively, if all we knew about a house was the neighborhood it was in, we would predict the average price of houses in that neighborhood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw6vvpP3Qgaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_housing.groupby(\"Neighborhood\")[\"SalePrice\"].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ler3YwSNQga6",
        "colab_type": "text"
      },
      "source": [
        "These numbers match the predictions from our linear regression model exactly. Linear regression simply predicts the average price in each neighborhood. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4cCuhK6Qga8",
        "colab_type": "text"
      },
      "source": [
        "To see this mathematically, recall that linear regression minimizes the total squared distance between the observed price and the predicted price:\n",
        "\n",
        "$$ \\text{sum of } (\\text{price} - \\widehat{\\text{price}})^2. $$\n",
        "\n",
        "After we expand the **Neighborhood** column into 28 dummy variables (e.g., $I\\{ \\text{Blmngtn} \\}$, $I\\{ \\text{Blueste} \\}$, etc.), one for each neighborhood, we can write the predicted price in the linear regression model as \n",
        "\n",
        "$$ \\widehat{\\text{price}} = c_1 I\\{ \\text{Blmngtn} \\} + c_2 I\\{ \\text{Blueste} \\} + \\ldots + c_{28} I\\{ \\text{Veenker} \\}. $$\n",
        "\n",
        "(For simplicity, we have omitted the intercept term $b$.)\n",
        "\n",
        "Now, consider a house in Bloomington Heights, for which $I\\{ \\text{Blmngtn} \\} = 1$ and all of the other dummy variables $I\\{ \\text{Blueste} \\} = \\ldots = I\\{ \\text{Veenker} \\} = 0$. Then, $\\widehat{\\text{price}}$ for a house in Bloomington Heights is $c_1$. Likewise, $\\widehat{\\text{price}}$ for a house in Bluestem is $c_2$. And so forth.\n",
        "\n",
        "Now, we can reframe linear regression as learning the values $c_1, c_2, \\ldots, c_{28}$ that minimize\n",
        "\n",
        "$$ \\text{sum of } (\\text{price} - \\widehat{\\text{price}})^2 = \\underbrace{\\text{sum of } (\\text{price} - c_1)^2}_{\\text{over houses in Blmngtn}} + \\underbrace{\\text{sum of } (\\text{price} - c_2)^2}_{\\text{over houses in Blueste}} + \\ldots + \\underbrace{\\text{sum of } (\\text{price} - c_{28})^2}_{\\text{over houses in Veenker}}. $$ \n",
        "\n",
        "We saw in Chapter 3 that the value of $c$ that mimimizes the $\\text{sum of } (\\text{price} - c)^2$ is the mean of the prices. So $\\hat c_1$ will be the average price of houses in Bloomington Heights, $\\hat c_2$ the average price of houses in Bluestem, and so on. Since $\\hat c_1, \\hat c_2, \\ldots, \\hat c_{28}$ are also the predicted values for each neighborhood, this shows that linear regression will predict the average label in each category when there is only one categorical variable in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHpASyuCQga-",
        "colab_type": "text"
      },
      "source": [
        "Exercise 1 in this lesson asks you to investigate what $k$-nearest neighbors regression does in the same situation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKEd982iQga_",
        "colab_type": "text"
      },
      "source": [
        "## Mixing Quantitative and Categorical Features\n",
        "\n",
        "In general, we want to fit machine learning models that use a mix of both categorical and quantitative features. In this situation, we will want to apply the `OneHotEncoder` to only the categorical features. Scikit-learn provides a `ColumnTransformer` that allows us to selectively apply transformations to certain columns.\n",
        "\n",
        "For example, suppose we want to fit a $k$-nearest neighbors model to predict house price from quantitative features (square footage, number of bedrooms, number of full bathrooms) and categorical features (neighborhood, building type). We can use a `ColumnTransformer` to standardize the quantitative features and one-hot encode the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOlC1gUNQgbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "ct = make_column_transformer(\n",
        "    (StandardScaler(), [\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\"]),\n",
        "    (OneHotEncoder(), [\"Neighborhood\", \"Bldg Type\"]),\n",
        "    remainder=\"drop\"  # all other columns in X will be dropped.\n",
        ")\n",
        "ct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afelS7eJQgbF",
        "colab_type": "text"
      },
      "source": [
        "Next, we integrate this `ColumnTransformer` into a pipeline (refer to the previous lesson) with the `KNeighborsRegressor` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a01yODHQgbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ct,\n",
        "    KNeighborsRegressor(n_neighbors=10)\n",
        ")\n",
        "\n",
        "pipeline.fit(X=df_housing[[\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\",\n",
        "                           \"Neighborhood\", \"Bldg Type\"]], \n",
        "             y=df_housing[\"SalePrice\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_H2s5R3QgbM",
        "colab_type": "text"
      },
      "source": [
        "Now, if we wanted to use this model to predict the price of a 3BR/2BA, 1700 sqft single-family house in Bloomington Heights, we could create a `Series` with this information, and call `pipeline.predict()` on a 2D-array with this single row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__A47DO-QgbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = pd.Series()\n",
        "x_test[\"Gr Liv Area\"] = 1700\n",
        "x_test[\"Bedroom AbvGr\"] = 3\n",
        "x_test[\"Full Bath\"] = 2\n",
        "x_test[\"Neighborhood\"] = \"Blmngtn\"\n",
        "x_test[\"Bldg Type\"] = \"1Fam\"\n",
        "\n",
        "pipeline.predict(X=pd.DataFrame([x_test]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmHVpKhBQgbU",
        "colab_type": "text"
      },
      "source": [
        "So this house is predicted to cost $251,550."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO3FhdpTQgbV",
        "colab_type": "text"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0lC3xbFQgbW",
        "colab_type": "text"
      },
      "source": [
        "1\\. Using the Ames data set, build a $10$-nearest neighbors model to predict house price using **Neighborhood** as the only feature. How do the predictions compare with just using the mean house price of each neighborhood? If there are any discrepancies, can you explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_wV5j2_QgbX",
        "colab_type": "text"
      },
      "source": [
        "2\\. In the example from the lesson, we standardized the quantitative features and one-hot encoded the categorical features in parallel. This means that the dummy variables were not standardized before being passed into the $10$-nearest neighbors model. How would you modify the pipeline so that *all* of the variables are standardized?\n",
        "\n",
        "(_Hint:_ You may find the `remainder=\"passthrough\"` option of `ColumnTransformer` helpful.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFxX8W2FQgbb",
        "colab_type": "text"
      },
      "source": [
        "3\\. Using the tips data set (http://dlsun.github.io/pods/data/tips.csv ), use a $5$-nearest neighbors model to predict how much a male diner will tip on a Sunday bill of \\$40.00."
      ]
    }
  ]
}