{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "6.3 Estimating Test Metrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/pods/blob/master/06-Classification-Models/6.3%20Estimating%20Test%20Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLq7mGD6kjMd",
        "colab_type": "text"
      },
      "source": [
        "# 6.3 Estimating Test Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmKrVx3pkjMl",
        "colab_type": "text"
      },
      "source": [
        "In the previous lesson, we learned several scores (accuracy, precision, recall, F1) for evaluating classification models. We calculated these scores on the training data---that is, the same data that was used to evaluate the model. In Chapter 5, we saw that evaluating machine learning models on the training data is problematic because a machine learning model can achieve a good training score by _overfitting_ to the training data. We argued that the goal of a machine learning model should be to achieve a good score on test data. (Chapter 5.4) However, ground truth labels are often not available for the test data. Nevertheless, we can use cross-validation on the training data to estimate the test scores. (Chapter 5.5) These so-called _validation scores_ can be used to select between models and tune hyperparameters. (Chapter 5.6)\n",
        "\n",
        "Although Chapter 5 was about regression models, the exact same program can be carried out for classification models. Instead of calculating the *training* accuracy, precision, etc., we estimate the *test* accuracy, precision, etc. using cross-validation. This lesson demonstrates how to carry out this program, but the concepts (and even the code) are essentially identical to Chapter 5.\n",
        "\n",
        "First, we define a classifier that we want to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCtQkyAokjMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "data_dir = \"http://dlsun.github.io/pods/data/\"\n",
        "df_breast = pd.read_csv(data_dir + \"breast-cancer.csv\")\n",
        "\n",
        "X_train = df_breast[[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\n",
        "                     \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\",\n",
        "                     \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\"]]\n",
        "y_train = df_breast[\"Class\"]\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KNeighborsClassifier(n_neighbors=10)\n",
        ")\n",
        "\n",
        "pipeline.fit(X=X_train, y=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzRWgQ7qkjMy",
        "colab_type": "text"
      },
      "source": [
        "To calculate test scores using $k$-fold cross validation, we use the `cross_val_score` function in scikit-learn. For example, to calculate test accuracy, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ4TfB4SkjM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
        "                            cv=10, scoring=\"accuracy\")\n",
        "cv_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8891bSQPkjM6",
        "colab_type": "text"
      },
      "source": [
        "We get 10 accuracy scores, one from each of the $k=10$ folds. It is customary to average these accuracy scores to obtain one overall estimate of the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lzoByBGkjM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_scores.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gghNK3x3kjNC",
        "colab_type": "text"
      },
      "source": [
        "This validation accuracy is high, but lower than the 97.2% training accuracy that we obtained in the previous lesson. This makes sense because it is always harder for a model to predict on data it has not seen than on data it saw. Recall that Wenger's neural network model that won the Google Science Fair had an accuracy of 97.4%. We have come close to achieving that using a simple $10$-nearest neighbors classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlHU2vkMkjNF",
        "colab_type": "text"
      },
      "source": [
        "Scikit-Learn can also calculate the precision and recall of a class $c$, but the labels need to be converted to a binary label that is $1$ (or `True`) if the observation is in class $c$ and $0$ (or `False`) otherwise. For example, to calculate the precision for benign tumors (class 0), we define the new label `is_benign`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFe5IG5ukjNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_benign = (y_train == 0)\n",
        "\n",
        "cross_val_score(pipeline, X_train, is_benign, \n",
        "                cv=10, scoring=\"precision\").mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjpoZ8iVkjNf",
        "colab_type": "text"
      },
      "source": [
        "To calculate the validation _recall_ for benign tumors, we simply change the scoring method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJc9Gy45kjNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_score(pipeline, X_train, is_benign, \n",
        "                cv=10, scoring=\"recall\").mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9kzEeuVkjN1",
        "colab_type": "text"
      },
      "source": [
        "Likewise, the validation precision and recall for malignant tumors is "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X29ZE0KekjN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_malignant = (y_train == 1)\n",
        "\n",
        "precision = cross_val_score(pipeline, X_train, is_malignant, \n",
        "                            cv=10, scoring=\"precision\").mean()\n",
        "recall = cross_val_score(pipeline, X_train, is_malignant, \n",
        "                         cv=10, scoring=\"recall\").mean()\n",
        "\n",
        "precision, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkLKAD4fkjN9",
        "colab_type": "text"
      },
      "source": [
        "Another term for recall is _sensitivity_. Wenger's model was 99.1% sensitive to malignancy; our model is quite a bit worse, achieving a sensitivity (i.e., recall) of only 93.7%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlfLbhqWkjN-",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Could we do better with a different value of $k$? We can use cross-validation on a grid of $k$ values and pick the one that maximizes some score. Since the F1 score summarizes both precision and recall, we use F1 as the score. There are two F1 scores---one for the benign masses and the malignant masses---`_macro` specifies that we take the average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULyEVdT1kjN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid={\"kneighborsclassifier__n_neighbors\": range(1, 50)},\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6uXKGD6kjOM",
        "colab_type": "text"
      },
      "source": [
        "Is this value of $k$ better? It certainly has a higher average F1 score. What about its precision and recall for malignant masses?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_PygmI0kjOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_precision = cross_val_score(\n",
        "    grid_search.best_estimator_,\n",
        "    X_train, is_malignant,\n",
        "    scoring=\"precision\",\n",
        "    cv=10).mean()\n",
        "\n",
        "new_recall = cross_val_score(\n",
        "    grid_search.best_estimator_,\n",
        "    X_train, is_malignant,\n",
        "    scoring=\"recall\",\n",
        "    cv=10).mean()\n",
        "\n",
        "precision, new_precision, recall, new_recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qslknI2kjOW",
        "colab_type": "text"
      },
      "source": [
        "We see that the new model has a higher precision _and_ a higher recall for malignancy. This suggests that the new model is unambiguously better. (If only recall had been higher, then it could be argued that we were simply trading off precision for recall.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Iu-iEBokjOY",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "Exercises 1-2 ask you to use the Titanic data set (`https://dlsun.github.io/pods/data/titanic.csv`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThXrjLY5kjOZ",
        "colab_type": "text"
      },
      "source": [
        "1\\. Train a 5-nearest neighbors model to predict whether or not a passenger on a Titanic survived, using their age, sex, and class as features. Estimate the test accuracy, precision, and recall of this model for the survivors and the deceased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFDd1tF7kjOa",
        "colab_type": "text"
      },
      "source": [
        "2\\. You want to build a $k$-nearest neighbors model to predict whether or not a passenger on the Titanic survived, using their age, sex, and class. \n",
        "\n",
        "- What value of $k$ optimizes overall accuracy?\n",
        "- What value of $k$ optimizes the F1 score for the deceased?\n",
        "\n",
        "Does the same value of $k$ optimize accuracy and the F1 score?"
      ]
    }
  ]
}