{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dlsun/pods/blob/master/10-Textual-Data/10.2%20The%20Vector%20Space%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYVDhxtNb74o"
   },
   "source": [
    "# 10.2 The Vector Space Model\n",
    "\n",
    "In the previous section, we saw how a single document could be converted into a _bag of words_ (or, more precisely, a bag of $n$-grams) representation. In this section, we go one step further, converting an entire corpus of documents into tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0WK4Y2qb74r"
   },
   "source": [
    "## Term Frequencies\n",
    "\n",
    "The bag of words representation gives us a mapping between words and their counts, such as `{..., \"am\": 3, \"i\": 71, \"sam\": 6, ...}`. To turn the bag of words into a vector of numbers, we can simply take the word counts, as follows:\n",
    "\n",
    "| ... | i  | am | sam | ... |\n",
    "|-----|----|----|-----|-----|\n",
    "| ... | 71 |  3 |  6  | ... |\n",
    "\n",
    "If we do this for each document in the corpus, and stack the rows, we obtain a table of numbers called the _term-frequency matrix_. \n",
    "\n",
    "|        | ... | i  | am | sam | ... |\n",
    "|--------|-----|----|----|-----|-----|\n",
    "|**green_eggs_and_ham**| ... | 71 |  3 |  6  | ... |\n",
    "|**cat_in_the_hat**| ... | 59 | 0 | 0 | ... |\n",
    "|**fox_in_socks**| ... | 13 | 0 | 0 | ... |\n",
    "|...|...|...|...|...|...|\n",
    "|**one_fish_two_fish**| ... | 51 | 3 | 0 | ... |\n",
    "\n",
    "The columns are all words (or _terms_) that appear in the corpus, which collectively make up the _vocabulary_. The idea of representing documents by a vector of numbers is called the _vector space model_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cgl6jLw3cErP"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's obtain the term-frequency matrix for the Dr. Seuss books. First, we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7Vy5COKb74s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "seuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\n",
    "seuss_files = [\n",
    "    \"green_eggs_and_ham.txt\", \"cat_in_the_hat.txt\", \"fox_in_socks.txt\",\n",
    "    \"hop_on_pop.txt\", \"horton_hears_a_who.txt\", \"how_the_grinch_stole_christmas.txt\",\n",
    "    \"oh_the_places_youll_go.txt\", \"one_fish_two_fish.txt\"\n",
    "]\n",
    "\n",
    "docs_seuss = pd.Series()\n",
    "for file in seuss_files:\n",
    "    response = requests.get(seuss_dir + file, \"r\")\n",
    "    docs_seuss[file[:-4]] = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UOASR79b74x"
   },
   "source": [
    "Now we apply the bag of words representation to the normalized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6VrqvvJb74y"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = (\n",
    "    docs_seuss.\n",
    "    str.lower().                  # convert all letters to lowercase\n",
    "    str.replace(\"[^\\w\\s]\", \" \").  # replace non-alphanumeric characters by whitespace\n",
    "    str.split()                   # split on whitespace\n",
    ").apply(Counter)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvgU4114b742"
   },
   "source": [
    "To turn this into a term-frequency matrix, we need to make a `DataFrame` out of it, where each column represents a word and each row a document---and each entry is the count of that word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylxYncocb744"
   },
   "outputs": [],
   "source": [
    "tf = pd.DataFrame(list(bag_of_words))\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAYV8ZPtb747"
   },
   "source": [
    "This matrix is full of missing numbers. A missing number means that the word did not appear in that document. In other words, a count of `NaN` really means a count of 0. So it makes sense in this situation to replace the `NaN`s by 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p9mciwPb748"
   },
   "outputs": [],
   "source": [
    "tf = tf.fillna(0)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbTiSkqNb75B"
   },
   "source": [
    "### Implementation using `scikit-learn`\n",
    "\n",
    "We could have also used the `CountVectorizer` in `scikit-learn` to obtain the term-frequency matrix. This vectorizer is fit to a list of the documents in the corpus. By default, it converts all letters to lowercase and strips punctuation, although this behavior can be customized using the `strip_accents=` and `lowercase=` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhl2Kwb5b75C"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit(docs_seuss) # This determines the vocabulary.\n",
    "tf_sparse = vec.transform(docs_seuss)\n",
    "\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ni61EAub75G"
   },
   "source": [
    "Notice that `CountVectorizer` returns the term-frequency matrix, not as a `DataFrame` or even as a `numpy` array, but as a `scipy` sparse matrix. A _sparse matrix_ is one whose entries are mostly zeroes. For example,\n",
    "\n",
    "$$ \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 1.7 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -0.8 & 0 \\end{pmatrix} $$\n",
    "\n",
    "is an example of a sparse matrix. Instead of storing 20 values (most of which are equal to 0), we can simply store the locations of the non-zero entries and their values:\n",
    "\n",
    "- $(1, 0) \\rightarrow 1.7$\n",
    "- $(3, 3) \\rightarrow -0.8$\n",
    "\n",
    "All other entries of the matrix are assumed to be zero. This representation offers substantial memory savings when there are only a few non-zero entries. (But if not, then this representation can actually be more expensive.) Term-frequency matrices are usually sparse because most words do not appear in all documents.\n",
    "\n",
    "The `scipy` sparse matrix format is used to store sparse matrices. If necessary, a `scipy` sparse matrix can be converted to a `numpy` matrix using the `.todense()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61q9l5bbb75H"
   },
   "outputs": [],
   "source": [
    "tf_sparse.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibRYltDdb75L"
   },
   "source": [
    "We can further convert this `numpy` matrix to a `pandas` `DataFrame`. To make the column names descriptive, we call the `.get_feature_names()` method of the `CountVectorizer`, which returns a list of the words in the order that they appear in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKAtlcEDb75M"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    tf_sparse.todense(),\n",
    "    columns=vec.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnompYDTb75P"
   },
   "source": [
    "The term-frequency matrix that `CountVectorizer` produced is not exactly the same as the matrix that we produced ourselves using just `pandas`. Although the two matrices have the same number of rows (8, corresponding to the number of documents in the corpus), they have a different number of columns. It appears that `CountVectorizer` had a vocabulary that was 11 words smaller (1344 words instead of 1355). We can determine exactly which 11 words these are, by taking the set difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK31TXDvb75Q"
   },
   "outputs": [],
   "source": [
    "set(tf.columns) - set(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tF1gsPqb75U"
   },
   "source": [
    "We see that all of the words that `CountVectorizer` missed were one-character long. By default, `CountVectorizer` only retains words that are at least 2 characters long. This behavior can be customized using the `token_pattern=` parameter, but we will not pursue that here, since 1-letter words are usually not useful for analysis anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5Pt4YzPb75V"
   },
   "source": [
    "`CountVectorizer` can even count $n$-grams. If we wanted both unigrams (i.e., individual words) and bigrams, then we would specify `ngram_range=(1, 2)`. If we wanted only the bigrams, then we would specify `ngram_range=(2, 2)`. \n",
    "\n",
    "Let's get unigrams, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXPmH0rPb75W"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 3))\n",
    "vec.fit(docs_seuss)\n",
    "vec.transform(docs_seuss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRWacCYmb75Z"
   },
   "outputs": [],
   "source": [
    "# number of non-zero values in the sparse matrix.\n",
    "vec.transform(docs_seuss).count_nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbOmJw3Hb75c"
   },
   "source": [
    "There are nearly 15,000 bigrams. If we wanted to store this data in a `DataFrame`, we would need as many columns, even though only about 16,000 out of the nearly 120,000 entries are nonzero. This is why sparse matrices are vital in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvqfLNhNb75d"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "The problem with term frequencies (TF) is that common words like \"the\" and \"that\" tend to have high counts and dominate. A better indicator of whether two documents are similar is if they share rare words. For example, the word \"eat\" only appears in two of the Dr. Seuss stories. The presence of that word in two documents is a strong indicator that the documents are similar, so we should give more weight to terms like it.\n",
    "\n",
    "This is the idea behind TF-IDF. We take each term frequency and re-weight it according to how many documents that term appears in (i.e., the **document frequency**). Since we want words that appear in fewer documents to get more weight, we take the **inverse document frequency** (IDF).  We take the logarithm of IDF because the distribution of IDFs is heavily skewed to the right. (Remember the discussion about transforming data from Chapter 1.4.) So in the end, the formula for IDF is:\n",
    "\n",
    "$$ \\textrm{idf}(t, D) = \\log \\frac{\\text{# of documents}}{\\text{# of documents containing $t$}} = \\log \\frac{|D|}{|d \\in D: t \\in d|}. $$\n",
    "\n",
    "(Sometimes, $1$ will be added to the denominator to prevent division by zero, if there are terms in the vocabulary that do not appear in the corpus.)\n",
    "\n",
    "To calculate TF-IDF, we simply multiply the term frequencies by the inverse document frequencies:\n",
    "\n",
    "$$ \\textrm{tf-idf}(d, t, D) = \\textrm{tf}(d, t) \\cdot \\textrm{idf}(t, D). $$\n",
    "\n",
    "Notice that unlike TF, the TF-IDF representation of a given document depends on the entire corpus of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7TZxq1jcP9A"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's first see how to calculate TF-IDF from scratch, using the term-frequency matrix we obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6-b9Lgdb75e"
   },
   "outputs": [],
   "source": [
    "# Get document frequencies \n",
    "# (How many documents does each word appear in?)\n",
    "df = (tf > 0).sum(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NP10ysjOb75i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get IDFs\n",
    "idf = np.log(len(tf) / df)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H38ZejUGb75m"
   },
   "outputs": [],
   "source": [
    "# Calculate TF-IDFs\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIhNpIHfb75q"
   },
   "source": [
    "### Implementation using `scikit-learn`\n",
    "\n",
    "We will not generally implement TF-IDF from scratch, like we did above. Instead, we will use Scikit-Learn's `TfidfVectorizer`, which operates similarly to `CountVectorizer`, except that it returns a matrix of the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fbu5oi7ib75r"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
    "vec.fit(docs_seuss) # This determines the vocabulary.\n",
    "tf_idf_sparse = vec.transform(docs_seuss)\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cycHn7Cjb75v"
   },
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "We now have a representation of each text document as a vector of numbers. Each number can either be a term frequency or a TF-IDF weight. We can visualize each vector as an arrow in a high-dimensional space, where each dimension represents a word. The magnitude of the vector along a dimension represents the \"frequency\" (TF or TF-IDF) of that word in the document. For example, if our vocabulary only contains two words, \"i\" and \"sam\", then the arrows shown below might represent two documents:\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "To fit $k$-nearest neighbors or $k$-means clustering, we need some way to measure the distance between two documents (i.e., two vectors). We could use Euclidean distance, as we have done in the past.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_euclidean.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "But Euclidean distance does not make sense for TF or TF-IDF vectors. To see why, consider the two documents:\n",
    "\n",
    "1. \"I am Sam.\" \n",
    "2. \"I am Sam. Sam I am.\" \n",
    "\n",
    "The two documents are obviously very similar. But the vector for the second is twice as long as the vector for the first because the second document has twice as many occurrences of each word. This is true whether we use TF or TF-IDF weights. If we calculate the Euclidean distance between these two vectors, then they will seem quite far apart.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_example.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "With TF and TF-IDF vectors, the distinguishing property is their _direction_. Because the two vectors above point in the same direction, they are similar. We need a distance metric that measures how different their directions are. A natural way to measure the difference between the directions of two vectors is the angle between them.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_cosine.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "The cosine of the angle between two vectors ${\\bf a}$ and ${\\bf b}$ can be calculated as:\n",
    "\n",
    "$$ \\cos \\theta = \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$\n",
    "\n",
    "Although it is possible to work out the angle $\\theta$ from this formula, it is more common to report $\\cos\\theta$ as a measure of similarity between two vectors. This similarity metric is called **cosine similarity**. Notice that when the angle $\\theta$ is close to 0 (i.e., when the two vectors point in nearly the same direction), the value of $\\cos\\theta$ is high (close to 1.0, which is the maximum possible value).\n",
    "\n",
    "The cosine _distance_ is defined as 1 minus the similarity. This makes it so that 0 means that the two vectors point in the same direction:\n",
    "\n",
    "$$ d_{\\cos}({\\bf a}, {\\bf b}) = 1 - \\cos(\\theta({\\bf a}, {\\bf b})) = 1 - \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rQbvOkVcbvK"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's calculate the cosine similarity between document 0 (_Green Eggs and Ham_) and document 2 (_Fox in Socks_) using the TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kagfHiKb75w"
   },
   "outputs": [],
   "source": [
    "# Calculate the numerator.\n",
    "a = tf_idf_sparse[0, :]\n",
    "b = tf_idf_sparse[2, :]\n",
    "dot = a.multiply(b).sum()\n",
    "\n",
    "# Calculate the terms in the denominator.\n",
    "a_len = np.sqrt(a.multiply(a).sum())\n",
    "b_len = np.sqrt(b.multiply(b).sum())\n",
    "\n",
    "# Cosine similarity is their ratio.\n",
    "dot / (a_len * b_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bqj82-h2b750"
   },
   "source": [
    "These two vectors are not very similar, as evidenced by their low cosine similarity (close to 0.0). Let's try to find the most similar documents in the corpus to _Green Eggs and Ham_---in other words, its nearest neighbors. To do this, we will take advantage of _broadcasting_: we will multiply a TF-IDF vector (representing document 0) by the entire TF-IDF matrix and calculate the sum over the columns. This will give us a vector of dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Nr27ArFb751"
   },
   "outputs": [],
   "source": [
    "# Calculate the numerators.\n",
    "a = tf_idf_sparse[0, :]\n",
    "B = tf_idf_sparse\n",
    "dot = a.multiply(B).sum(axis=1)\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tuXVdFAb754"
   },
   "outputs": [],
   "source": [
    "# Calculate the denominators.\n",
    "a_len = np.sqrt(a.multiply(a).sum())\n",
    "b_len = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "print(a_len)\n",
    "b_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m72blwPNb76A"
   },
   "outputs": [],
   "source": [
    "# Calculate their ratio to obtain cosine similarities.\n",
    "dot / (a_len * b_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEuizyNvb76E"
   },
   "source": [
    "Now let's put this matrix into a `DataFrame` so that we can easily sort the values in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IWk1cYEb76E"
   },
   "outputs": [],
   "source": [
    "cos_similarities = pd.DataFrame(dot / (a_len * b_len))[0]\n",
    "most_similar = cos_similarities.sort_values(ascending=False)\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELG94ygFb76H"
   },
   "source": [
    "Of course, the most similar document in the corpus to _Green Eggs and Ham_ (with a perfect cosine similarity of 1.0) is itself. But the next most similar text is _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBVAM4hmb76I"
   },
   "source": [
    "### Implementation using scikit-learn\n",
    "\n",
    "It is also possible to calculate cosine similarities/distances in `scikit-learn` using the same API that we used to calculate distances in Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZoMACMMb76J"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "cosine_similarity(tf_idf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTWQae5xb76M"
   },
   "source": [
    "The $(i, j)$th entry of this matrix represents the cosine similarity between the $i$th and $j$th documents. So the first row of this matrix contains the similarities between _Green Eggs and Ham_ and the other documents in the corpus. Check that these numbers match the ones we obtained manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vm7f6Jncb76N"
   },
   "outputs": [],
   "source": [
    "cosine_similarity(tf_idf_sparse)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMu61Qx5b76Q"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBojZWgHb76R"
   },
   "source": [
    "1\\. Suppose we had instead compared documents using cosine similarity on the term frequencies (TF), instead of TF-IDF. Does this change the conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEupF68-b76R"
   },
   "source": [
    "2\\. Suppose we had instead used Euclidean distance on the TF-IDF weights, instead of cosine distance. Does this change the conclusion? What if we first normalize the length of the TF-IDF vector for each document before calculating Euclidean distance?\n",
    "\n",
    "_Challenge Exercise:_ Can you prove the above fact mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_jiS5xzpWx2"
   },
   "source": [
    "3\\. Convert the self-summary variable (`essay0`) in the OKCupid data set (https://dlsun.github.io/pods/data/okcupid.csv) to a TF-IDF representation. Use this to find a match for user 61 based on what he says he is looking for in a partner (`essay9`).\n",
    "\n",
    "The [data dictionary](https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook.txt) may help you understand what the columns mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JUuovrSb76S"
   },
   "source": [
    "Exercises 4-5 ask you to work with the Enron spam data set (https://dlsun.github.io/pods/data/enron_spam.csv). This data set contains the subjects and bodies of a sample of e-mails that the Federal Energy Regulatory Commission (FERC) collected during their 2002 investigation of the energy company Enron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiqRT9Vxb76V"
   },
   "source": [
    "4\\. Each e-mail has additionally been manually labeled as spam (1) or not (0). Find the TF-IDF representation of the bodies of the e-mails. Which e-mail was most similar to e-mail 0 (not spam)? Which e-mail was most similar to e-mail 1001 (spam)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iy7hIjLDb76W"
   },
   "source": [
    "5\\. (This exercise requires material from Part II of this book.) Write a function `predict_spam()` that accepts an e-mail body and predicts whether or not it is spam using $k$-nearest neighbors on the Enron spam data set. Use cosine distance ($= 1 - \\text{cosine similarity}$) as your distance metric.\n",
    "\n",
    "Use your model to predict whether an e-mail with the body \"free cash\" is spam or not."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "10.2 The Vector Space Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
