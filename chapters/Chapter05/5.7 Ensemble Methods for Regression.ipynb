{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Regression\n",
    "\n",
    "In the previous lesson, we learned how to choose between competing machine learning models. However, it is usually better to combine the predictions from the models than to use the prediction from the single best model. This lesson describes methods for combining predictions from different regression models. Methods for combining predictions from different machine learning models are called _ensemble methods_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_dir = \"http://dlsun.github.io/pods/data/\"\n",
    "df_housing = pd.read_csv(data_dir + \"AmesHousing.txt\", sep=\"\\t\")\n",
    "\n",
    "X_train = df_housing[[\"Gr Liv Area\", \"Bedroom AbvGr\", \"Full Bath\"]]\n",
    "y_train = df_housing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fit two models to predict house price: a linear regression model and a $10$-nearest neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Train linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Train 10-nearest neighbors model\n",
    "knn_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(n_neighbors=10)\n",
    ")\n",
    "knn_model.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Predict for a 1500 sqft, 2BR/2BA house\n",
    "new_house = [[1500, 2, 2]]\n",
    "linear_model.predict(new_house), knn_model.predict(new_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$-nearest neighbors model is more optimistic than the linear regression model. How should we combine these two predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "The simplest ensemble method is _voting_. To apply voting to regression models, we simply average the predictions from the different models. So the predicted quality of the 1990 vintage is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean([\n",
    "    linear_model.predict(new_house),\n",
    "    knn_model.predict(new_house)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the voting ensemble in scikit-learn, using `VotingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "ensemble_model = VotingRegressor([\n",
    "    (\"linear\", linear_model), \n",
    "    (\"knn\", knn_model)\n",
    "])\n",
    "ensemble_model.fit(X=X_train, y=y_train)\n",
    "ensemble_model.predict(new_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this ensemble model? Is it better than each model individually? We let cross-validation decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for model in [linear_model, knn_model, ensemble_model]:\n",
    "    print(-cross_val_score(model, X=X_train, y=y_train, cv=5,\n",
    "                           scoring=\"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these errors are large because the metric is the mean *squared* error (MSE). To obtain a more interpretable metric, we can take the square root to obtain the RMSE. However, for the purposes of determining which model is best, we just need to know which value is smallest. The ensemble model has a lower MSE than either model individually, even though it simply averages the predictions from the two models. This example illustrates the power of ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "In voting, every model's prediction gets equal weight (or at least a predefined weight, which can be specified using the `weights=` parameter of `VotingRegressor`). Another approach, called _stacking_ , learns the weights $c_1, \\ldots, c_M$ given to each of the $M$ models so that the predicted label \n",
    "\n",
    "$$ \\hat y = c_1 \\hat y_1 + \\ldots + c_M \\hat y_M $$\n",
    "\n",
    "is as close to the true label as possible. \n",
    "\n",
    "We already know how to fit a model of this form. We fit linear regression to predict the same label $y$, but the features are now the predictions $\\hat y_1, \\ldots, \\hat y_M$ from the individual models. \n",
    "\n",
    "For example, to learn the weights that should be given to the linear regression and 10-nearest neighbors models above, we first stack the predictions into an array with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([\n",
    "    linear_model.predict(X_train), knn_model.predict(X_train)\n",
    "], axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a linear regression model on these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = LinearRegression()\n",
    "stacker.fit(X=preds, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of stacking above, while conceptually correct, is not quite accurate. In practice, to prevent overfitting, each prediction in `preds` is calculated using a model that was trained on different data. This can be accomplished using an approach similar to cross-validation, but the details are beyond the scope of this book. Fortunately, scikit-learn handles most of the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "stacking_model = StackingRegressor([\n",
    "    (\"linear\", linear_model), \n",
    "    (\"knn\", knn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "stacking_model.fit(X=X_train, y=y_train)\n",
    "\n",
    "stacker = stacking_model.final_estimator_\n",
    "stacker.intercept_, stacker.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the combined prediction is\n",
    "\n",
    "$$ \\widehat{\\text{SalePrice}} = 290.5 + 0.30 \\cdot (\\text{linear regression prediction}) + 0.70 \\cdot (\\text{$10$-nearest neighbors prediction}). $$\n",
    "\n",
    "Contrast this with voting, which gave equal weight (i.e., $0.50$) to the two predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does stacking do? Again, we use cross-validation to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-cross_val_score(stacking_model, X=X_train, y=y_train, cv=5,\n",
    "                 scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best model we've seen so far. The error is even lower than the voting ensembler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Intuition for Ensemble Methods\n",
    "\n",
    "Why do ensemble methods work so well? Let's consider a model with just one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.plot.scatter(x=\"Gr Liv Area\", y=\"SalePrice\", color=\"gray\")\n",
    "\n",
    "X_train = df_housing[[\"Gr Liv Area\"]]\n",
    "y_train = df_housing[\"SalePrice\"]\n",
    "\n",
    "for model in [LinearRegression(), \n",
    "              KNeighborsRegressor(n_neighbors=10), \n",
    "              ensemble_model]:\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    X_test = pd.DataFrame({\"Gr Liv Area\": np.arange(6000)})\n",
    "    predictions = pd.Series(\n",
    "        model.predict(X_test),\n",
    "        index=X_test[\"Gr Liv Area\"]\n",
    "    )\n",
    "    predictions.plot.line(legend=True, label=type(model).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ensemble model combines the best features of $k$-nearest neighbors and linear regression.\n",
    "\n",
    "- Like $k$-nearest neighbors, it is able to model nonlinear relationships (such as the pattern near 3000 sqft).\n",
    "- Like linear regression, it is able to extrapolate near the boundaries of the data range. (Contrast this with $k$-nearest neighbors, which produces predictions that are constant near the boundaries.)\n",
    "\n",
    "This is why the ensemble model is often superior to each model individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Exercise 1-2 asks you to use the tips data set (http://dlsun.github.io/pods/data/tips.csv )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. In Chapter 5.1, you trained a linear regression model to predict the tip from the gender, day of the week, and the total bill. In Chapter 5.2, you trained a $k$-nearest neighbors model to do the same. (In Chapter 5.6, you determined the optimal value of $k$.) Combine these two models using voting. Is the ensemble method better than each method individually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now combine the two models using stacking. How does stacking compare to voting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
